{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training dataset for ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "sys.path.append('../../ELEP/src/')\n",
    "from mbf_utils import make_LogFq, make_LinFq, rec_filter_coeff, create_obspy_trace\n",
    "from mbf import MB_filter as MBF\n",
    "\n",
    "# If you use multiprocessors, please install mpi4py and run it with mpirun locally\n",
    "# from mpi4py import MPI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare raw data \n",
    "flnm = ['Instance_snr_18_20_testing.csv']\n",
    "dirpath_csv  = # define the path to the metadata in .csv file \n",
    "dirpath_data = # define the path to the waveforms in .h5 file\n",
    "\n",
    "dirpath_csv_save = # define the path to save the metadata in .csv file\n",
    "dirpath_data_save = # define the path to save the waveforms in .h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ifl in range (rank,len(flnm_list),size):\n",
    "    csv_fl = flnm_list[ifl]\n",
    "    print(f'rank={rank}, working on file={csv_fl}')\n",
    "\n",
    "    # write into a new h5\n",
    "    dirpath_csv = '/Volumes/GD-cyuan/DataYuan/INSTANCE/dataset/processed/Ensemble/noisy_INSTANCE/'\n",
    "    dirpath_data = '/Volumes/GD-cyuan/DataYuan/INSTANCE/dataset/processed/Ensemble/INSTANCE_data/'\n",
    "\n",
    "    # set up parameters for multi-band frequency filtering\n",
    "    npts, dt, fs, nc, mtraces = 6000, 0.01, 100., 3, 20000\n",
    "    components = ['E','N','Z']\n",
    "    MBF_filter_paras = {'f_min':1, 'f_max':45, 'nfbs':10, 'frequencies':[], 'dt':0.01, 'CN_HP':[], 'CN_LP':[], 'npoles':2} \n",
    "    MBF_filter_paras['frequencies'] = make_LogFq(MBF_filter_paras['f_min'], MBF_filter_paras['f_max'], MBF_filter_paras['dt'], MBF_filter_paras['nfbs'])\n",
    "    MBF_filter_paras['CN_HP'], MBF_filter_paras['CN_LP'] = rec_filter_coeff(MBF_filter_paras['frequencies'], MBF_filter_paras['dt'])\n",
    "    freq_list = MBF_filter_paras['frequencies']\n",
    "    MBF_filter_paras['detection_threshold'], MBF_filter_paras['P_threshold'], MBF_filter_paras['S_threshold'] = 0.3, 0.1, 0.1\n",
    "    MBF_filter_paras['estimate_uncertainty'] = None\n",
    "    nfbs = len(freq_list-1)\n",
    "    print(freq_list)\n",
    "\n",
    "    snr = int(csv_fl.split('_')[-2])\n",
    "    dirpath_wav = '/Volumes/GD-cyuan/DataYuan/INSTANCE/dataset/processed/metadata_STEAD_events_STEAD_noise_snr0_20_full.hdf5'\n",
    "\n",
    "    # change name with csv file name\n",
    "    pick_prob_fnm_mbf = dirpath_data+csv_fl[:-4]+'_mbf_wav.hdf5'\n",
    "\n",
    "    h5_wdata2 = h5py.File(pick_prob_fnm_mbf, 'a')\n",
    "    h5_wdata2.create_group('data') \n",
    "\n",
    "    # open h5 reader\n",
    "    h5_rdata = h5py.File(dirpath_wav, 'r')\n",
    "\n",
    "    # read csv for trace names\n",
    "    csv_reader = pd.read_csv(dirpath_csv+csv_fl)\n",
    "    trace_name_list = csv_reader.trace_name.values\n",
    "\n",
    "    # start prediction\n",
    "    for itr, trnm in enumerate(trace_name_list[:mtraces]): \n",
    "        if itr%2000 == 0:\n",
    "            print(f'working on itr={itr}')\n",
    "    \n",
    "        # get data\n",
    "        dataset = h5_rdata.get('data/'+str(trnm)) \n",
    "        data = np.array(dataset)\n",
    "\n",
    "        # MBF filtering\n",
    "        mbf_clear_data = np.zeros([nfbs, npts, nc])\n",
    "        for ic in range(nc):\n",
    "            mbf_clear_data[:, :, ic] = MBF(data[ic], MBF_filter_paras)\n",
    "      \n",
    "        # save data\n",
    "        h5_wdata2 = h5py.File(pick_prob_fnm_mbf, 'r')\n",
    "        h5w2 = h5_wdata2.create_dataset('data/'+trnm, mbf_clear_data.shape, data = mbf_clear_data, dtype= np.float32) \n",
    "        h5w2.flush() \n",
    "\n",
    "    # h5_wdata.close()\n",
    "    h5_wdata2.close()\n",
    "    h5_rdata.close()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
