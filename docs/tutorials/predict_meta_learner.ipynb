{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo performing prediction of using meta-learner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "from ensemble_learners import ensemble_regressor_cnn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paras\n",
    "dirpath_data = '/mnt/Data02/DataDL/Ensemble/INSTANCE_prediction_02/original/training/CNN_regress/'\n",
    "dirpath_data2 = '/mnt/Data02/DataDL/Ensemble/INSTANCE_prediction_02/original/prediction/CNN_regress/'\n",
    "\n",
    "# list files\n",
    "flnm_list = [ 'Instance_events_counts_4pick_filt_snr_-2_2_testing.csv',\n",
    "            'Instance_events_counts_4pick_filt_snr_2_4_testing.csv',\n",
    "             'Instance_events_counts_4pick_filt_snr_4_6_testing.csv',\n",
    "             'Instance_events_counts_4pick_filt_snr_6_8_testing.csv',\n",
    "             'Instance_events_counts_4pick_filt_snr_8_10_testing.csv',\n",
    "             'Instance_events_counts_4pick_filt_snr_10_12_testing.csv',\n",
    "             'Instance_events_counts_4pick_filt_snr_12_14_testing.csv',\n",
    "             'Instance_events_counts_4pick_filt_snr_14_16_testing.csv',\n",
    "            'Instance_events_counts_4pick_filt_snr_16_18_testing.csv',\n",
    "            'Instance_events_counts_4pick_filt_snr_18_20_testing.csv',\n",
    "            'Instance_events_counts_4pick_filt_snr_20_25_testing.csv',\n",
    "            'Instance_events_counts_4pick_filt_snr_25_30_testing.csv',\n",
    "            'Instance_events_counts_4pick_filt_snr_30_35_testing.csv',\n",
    "            'Instance_events_counts_4pick_filt_snr_35_40_testing.csv',\n",
    "            'Instance_events_counts_4pick_filt_snr_40_100_testing.csv'\n",
    "        ]\n",
    "\n",
    "# dirs and parameters\n",
    "dirpath_csv = '/mnt/Data02/DataDL/Ensemble/noisy_INSTANCE_02/'\n",
    "dirpath_pred = '/mnt/Data02/DataDL/Ensemble/INSTANCE_prediction_02/original/'\n",
    "\n",
    "# load predictions\n",
    "mdlnm_list = [\"eqt_original\",\"eqt_ethz\",\"eqt_instance\",\"eqt_scedc\",\"eqt_stead\",\"eqt_neic\"]\n",
    "nmdls, nflnms = len(mdlnm_list), len(flnm_list)\n",
    "ntrace, nt, t, dt, nc, mc = 15000, 6000, 60, 0.01, 3, 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### perform prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ifl, flnm in enumerate(flnm_list[:]):\n",
    "    # initializing\n",
    "    pdata = np.zeros([ntrace,mc,nmdls,nt], dtype=np.float32)\n",
    "    trace_name_train_list, trace_stt_train_list, tp_sample_train_list, ts_sample_train_list = [], [], [], []\n",
    "    model_fnm = 'Instance_training_set_original_tp_L2k.pt'\n",
    "    valid_fnm = flnm[:-4]+'_prediction_tp_L2k'\n",
    "    print(f'working on valid_fnm={valid_fnm}')\n",
    "\n",
    "    # load csv\n",
    "    csv_reader = pd.read_csv(dirpath_csv+flnm)\n",
    "    trace_name_list = csv_reader['trace_name'].values\n",
    "    trace_stt_list = csv_reader['trace_start_time'].values\n",
    "    tp_sample_list = csv_reader['p_arrival_sample'].values\n",
    "    ts_sample_list = csv_reader['s_arrival_sample'].values\n",
    "\n",
    "    for imdl, mdlnm in enumerate(mdlnm_list):\n",
    "        print(f'working on imdl, ifl = {imdl}, {ifl}')\n",
    "        pred_npy = np.load(dirpath_pred+flnm[:-4]+'_pred_'+mdlnm_list[imdl]+'.npy')\n",
    "        pdata[:, 0, imdl, :] = pred_npy[:,1,:]\n",
    "  \n",
    "    # prepare data and labels\n",
    "    ntwin = 2000\n",
    "    cdata = np.zeros([ntrace, 1, nmdls, ntwin], dtype=np.float32) # Put P&S together, [N,C,H,W]\n",
    "    clabels = np.zeros([ntrace, 1], dtype=np.float32) # labels\n",
    "    for isamp in range(ntrace):\n",
    "        # extract manual/labled picks\n",
    "        itp, its = tp_sample_list[isamp], ts_sample_list[isamp]\n",
    "        # cut and write data\n",
    "        itind = random.randint(100, 1900)\n",
    "        if itp-itind+ntwin >= nt:\n",
    "            itind = itp+ntwin-nt\n",
    "        if itp-itind<0:\n",
    "            itind = itp\n",
    "        cdata[isamp] = pdata[isamp,0:1,:,itp-itind:itp-itind+ntwin]\n",
    "        # make label between 0 and 1\n",
    "        clabels[isamp] = itind/ntwin\n",
    "\n",
    "    # del pdata and free up memory\n",
    "    del pdata\n",
    "\n",
    "    # prepare dataloader\n",
    "    batch_size=500\n",
    "    shuffle = False # enforce to False\n",
    "    test_load=torch.utils.data.DataLoader(dataset=cdata, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    # load model\n",
    "    model = torch.load(dirpath_data+model_fnm)\n",
    "    model.eval()\n",
    "\n",
    "    # setup for prediction\n",
    "    CUDA = True\n",
    "    if CUDA:\n",
    "        model=model.cuda()\n",
    "    print('model is loaded and configured!')\n",
    "\n",
    "    # save validated dataset\n",
    "    test_data_arr = np.zeros(cdata.shape)\n",
    "    test_pred_arr = np.zeros(clabels.shape)\n",
    "    test_truth_arr = np.zeros(clabels.shape)\n",
    "    for i, batch in enumerate(test_load):\n",
    "        labels = torch.Tensor(clabels[i*batch_size:(i+1)*batch_size])\n",
    "        if CUDA:\n",
    "            batch =Variable(batch.cuda())\n",
    "            labels =Variable(labels.cuda())\n",
    "        else:\n",
    "            batch =Variable(batch)\n",
    "            labels =Variable(labels)\n",
    "\n",
    "        if i%2 == 0:\n",
    "            mtrace = (i+1)*batch_size\n",
    "            print(f'Prediction: completing {mtrace/ntrace:>3f}')\n",
    "\n",
    "        data = batch.detach().cpu().numpy()\n",
    "        pred = model(batch).detach().cpu().numpy()\n",
    "        truth = labels.detach().cpu().numpy()\n",
    "\n",
    "        test_data_arr[i*batch_size:(i+1)*batch_size] = data\n",
    "        test_pred_arr[i*batch_size:(i+1)*batch_size] = pred\n",
    "        test_truth_arr[i*batch_size:(i+1)*batch_size] = truth\n",
    "\n",
    "    # simple statistical calculation\n",
    "    misfit = test_pred_arr*ntwin/100 - test_truth_arr*ntwin/100\n",
    "    misfit_ave = np.mean(misfit[3000:])\n",
    "    misfit_mae = np.mean(np.abs(misfit[3000:]))\n",
    "    misfit_rmse = np.sqrt(np.mean(misfit[3000:]**2))\n",
    "    misfit_std = np.std(misfit[3000:])\n",
    "    misfit_median = np.median(misfit[3000:])\n",
    "    print(f'count, misfit_ave, misfit_mae, misfit_rmse, misfit_std = {len(misfit[3000:])}, {misfit_ave:>3f}, {misfit_mae:>3f}, {misfit_rmse:>3f}, {misfit_std:>3f}')\n",
    "\n",
    "    np.save(dirpath_data2+valid_fnm+'_data.npy', test_data_arr)\n",
    "    np.save(dirpath_data2+valid_fnm+'_pred.npy', test_pred_arr)\n",
    "    np.save(dirpath_data2+valid_fnm+'_truth.npy', test_truth_arr)\n",
    "    np.save(dirpath_data2+valid_fnm+'_stats.npy', np.array([misfit_ave,misfit_mae,misfit_rmse,misfit_std,misfit_median]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
